{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbdff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fc68bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False # Set to True if you want to see functions demo their output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf0c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f4a35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2d5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bab0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(model):\n",
    "    \"\"\"Sends model from CPU to CUDA.\"\"\"\n",
    "    model.cuda()\n",
    "    if isinstance(model, nn.Module):\n",
    "        for child in model.children():\n",
    "            to_cuda(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cad7363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    t = torch.randn(2,5)\n",
    "    t = t.to('cuda')\n",
    "    t.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1fba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paramaters\n",
    "num_blocks = 6\n",
    "MAX_SEQ_LEN = 20 # This results in seq_len+1 input rows. This is why positional encoding matrix needs seq_len +1 rows\n",
    "d_model = 12\n",
    "d_middle = 4*d_model\n",
    "d_token = d_model\n",
    "dropout = 0.1\n",
    "h = 6\n",
    "d_Q = d_model\n",
    "d_K = d_model\n",
    "d_V = d_model\n",
    "bs = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ab447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def create_upper_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def make_std_mask(tgt, pad):\n",
    "    \"Create a mask to hide padding and future words.\"\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(\n",
    "        create_upper_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "704b47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_upper_mask(MAX_SEQ_LEN + 2)\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    max_len = seq_len + 1\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                         -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cea74872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = positional_encoding(MAX_SEQ_LEN, d_model)\n",
    "pos_enc = pos_enc.to(device)\n",
    "if verbose:\n",
    "    print(pos_enc.shape)\n",
    "    print(pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816898e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(bs, seq_len, d_model):\n",
    "    r = torch.randint(1, d_model-1, (bs, seq_len))\n",
    "\n",
    "    enc_src, dec_src, target = r.clone(), r.clone(), r.clone()\n",
    "\n",
    "    et = (d_model-1)*torch.ones(bs, 1)\n",
    "    st = torch.zeros(bs,1)\n",
    "    \n",
    "    enc_src = torch.cat((enc_src, et),1)\n",
    "    dec_src = torch.cat((st, dec_src.flip(1)),1)\n",
    "    target = torch.cat((target.flip(1), et),1).long()\n",
    "\n",
    "    return enc_src, dec_src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f385326",
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    e,d,t = generate_batch(3, 4, 10)\n",
    "    print(e)\n",
    "    print(d)\n",
    "    print(t)\n",
    "    e = F.one_hot(e.to(torch.int64))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_middle):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_middle)\n",
    "        self.l2 = nn.Linear(d_middle, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.l2(F.relu(self.l1(x)))\n",
    "\n",
    "        return x \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30293dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d_model, d_Q, bias=False)\n",
    "        self.K = nn.Linear(d_model, d_K, bias=False)\n",
    "        self.V = nn.Linear(d_model, d_V, bias=False)\n",
    "        \n",
    "    def forward(self, q, k, v,  mask=None):\n",
    "        y = self.attention(self.Q(q), self.K(k), self.V(v), mask)\n",
    "        return y\n",
    "    \n",
    "    def attention(self, Q, K, V, mask = None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = F.softmax(scores, dim = -1)\n",
    "        attn = attn @ V\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29f400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(h*d_V, d_model, bias=False)     \n",
    "        self.a_modules = nn.ModuleList(AttentionModule(d_model, d_Q, d_K, d_V) for _ in range(h))\n",
    "            \n",
    "    def forward(self, q,k,v, mask=None):\n",
    "        combines = []\n",
    "\n",
    "        for layer in self.a_modules:\n",
    "            y = layer(q,k,v, mask)\n",
    "\n",
    "            combines.append(y)\n",
    "            \n",
    "        y = torch.cat(combines, -1)\n",
    "\n",
    "        y = self.linear(y)\n",
    "\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "461c8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_middle, dropout, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "\n",
    "        # multihead\n",
    "        self.multi_head = MultiHeadAttentionModule(d_model, h, d_Q, d_K, d_V)\n",
    "        # norm\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # feed forward\n",
    "        self.feed_forward = FeedForward(d_model, d_middle)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # multi head and skip and add\n",
    "        x = x + self.dropout(self.multi_head(x,x,x, mask))\n",
    "        # take the norm\n",
    "        x = self.norm1(x)\n",
    "        # feed forward and skip and add\n",
    "        x = x + self.dropout(self.feed_forward(x))\n",
    "        # take the norm\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8378d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, d_middle, dropout, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList(EncoderBlock(d_model, d_middle, dropout, h, d_Q, d_K, d_V) for _ in range(num_blocks))\n",
    "        \n",
    "    def forward(self, x, pe, mask=None):\n",
    "\n",
    "        x += pe\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53c7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27adb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_middle, dropout, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "\n",
    "        # multihead_masked\n",
    "        self.multi_head_masked = MultiHeadAttentionModule(d_model, h, d_Q, d_K, d_V)\n",
    "        # multihead_encoder\n",
    "        self.multi_head_encoder = MultiHeadAttentionModule(d_model, h, d_Q, d_K, d_V)\n",
    "        # norm\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # feed forward\n",
    "        self.feed_forward = FeedForward(d_model, d_middle)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x, y, mask):    \n",
    "        \"\"\"\n",
    "        x: decoder input\n",
    "        y: encoder output\n",
    "        \"\"\"  \n",
    "        #print(\"In decoder block forward x is:\", x)\n",
    "        x = x + self.dropout(self.multi_head_masked(x, x, x, mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.dropout(self.multi_head_encoder(x, y, y, None))\n",
    "        x = self.norm2(x) \n",
    "        x = x + self.dropout(self.feed_forward(x))\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db85b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, d_middle, d_token, dropout, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(DecoderBlock(d_model, d_middle, dropout, h, d_Q, d_K, d_V) for _ in range(num_blocks))\n",
    "        \n",
    "        self.l1 = nn.Linear(d_model, d_token)\n",
    "        \n",
    "    def forward(self, enc_out, dec_inp, pe, dec_mask=None):\n",
    "        x = dec_inp + pe\n",
    "        y = enc_out\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y, dec_mask)\n",
    "        x = self.l1(x)\n",
    "        #x = F.softmax(self.l1(x), -1) for crossentropy we do not want to take softmax\n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50d3ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, d_middle, d_token, dropout, h, d_Q, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_blocks, d_model, d_middle, dropout, h, d_Q, d_K, d_V)\n",
    "        self.decoder = Decoder(num_blocks, d_model, d_middle, d_token, dropout, h, d_Q, d_K, d_V)\n",
    "    \n",
    "    def forward(self, enc_src, dec_src, pe, enc_mask, dec_mask):\n",
    "        return self.decoder(self.encoder(enc_src, pe, enc_mask), dec_src, pe, dec_mask)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abdbbb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7540735",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_blocks, d_model, d_middle, d_token, dropout, h, d_Q, d_K, d_V)\n",
    "to_cuda(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86f8ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(transformer.parameters(), lr = 2.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b110729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt, bs, num_batches, d_model, pos_enc, mask, max_seq_len):\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    for batch_num in range(num_batches): \n",
    "\n",
    "        #enc_src, dec_src, target = generate_src(bs, seq_len, d_model)\n",
    "        seq_len = random.randint(1, max_seq_len)\n",
    "        enc_src, dec_src, target = generate_batch(bs, seq_len, d_model)\n",
    "        enc_src = F.one_hot(enc_src.to(torch.int64), num_classes=d_model).float()\n",
    "        dec_src = F.one_hot(dec_src.to(torch.int64), num_classes=d_model).float()\n",
    "\n",
    "        enc_src = enc_src.to(device)\n",
    "        dec_src = dec_src.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        pe = pos_enc[0,:seq_len+1,:d_model]\n",
    "        msk = mask[0, seq_len+1,: seq_len+1]\n",
    "        pred = transformer(enc_src, dec_src, pe, None, msk)\n",
    "\n",
    "        pred = pred.permute(0,2,1)\n",
    "        # Compute the loss\n",
    "        l = loss(pred, target)\n",
    "        total_loss += l.item()\n",
    "        # Backward pass\n",
    "        l.backward()\n",
    "        # Update the parameters\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if batch_num % 100 == 0 and batch_num != 0:\n",
    "            print(total_loss/500)\n",
    "            print(l)\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2c067ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.481101937294006\n",
      "tensor(2.3805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.373285518884659\n",
      "tensor(2.0092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.2910122692584993\n",
      "tensor(2.0114, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.226313214302063\n",
      "tensor(2.3473, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.248051732778549\n",
      "tensor(2.3201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.210751721858978\n",
      "tensor(2.2945, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.1603567504882815\n",
      "tensor(1.7048, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.0830362713336945\n",
      "tensor(2.1827, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.11541556596756\n",
      "tensor(1.1238, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "2.07302344083786\n",
      "tensor(2.2641, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.9729404884576798\n",
      "tensor(2.3078, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.9829325103759765\n",
      "tensor(2.2586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.9873683393001556\n",
      "tensor(2.2758, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.9198739284276962\n",
      "tensor(2.0664, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.8845225089788438\n",
      "tensor(2.0465, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.8402649080753326\n",
      "tensor(2.1019, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.8843025219440461\n",
      "tensor(1.6703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.7386790308356286\n",
      "tensor(0.6111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.7651623213291168\n",
      "tensor(1.9921, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.7675261169672012\n",
      "tensor(1.9227, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.8038662400841714\n",
      "tensor(2.1348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.7280448362231255\n",
      "tensor(2.0151, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.752849296927452\n",
      "tensor(1.7730, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.7161041778326034\n",
      "tensor(1.9925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.6176216420531273\n",
      "tensor(1.9945, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.6416405779123306\n",
      "tensor(1.4001, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.60730230987072\n",
      "tensor(1.6889, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.5073148222267627\n",
      "tensor(1.9457, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.468901174813509\n",
      "tensor(1.3938, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.46202016890049\n",
      "tensor(1.3632, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.4478029902279377\n",
      "tensor(1.6601, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.3551478698849677\n",
      "tensor(1.5563, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.2133606269955635\n",
      "tensor(1.8353, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.1788497407734395\n",
      "tensor(1.0440, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.0338962167501449\n",
      "tensor(0.4677, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "1.0227256181836129\n",
      "tensor(1.1019, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.899064225256443\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.9296792797744274\n",
      "tensor(0.3061, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.9154752413928509\n",
      "tensor(1.3476, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.7717655350267887\n",
      "tensor(0.3871, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.7466160308942199\n",
      "tensor(0.0852, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.7725639380514622\n",
      "tensor(0.2485, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.7102709593623877\n",
      "tensor(0.7596, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.5826803094148636\n",
      "tensor(0.4844, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.5755163291096688\n",
      "tensor(0.3656, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.5749068117141723\n",
      "tensor(0.2213, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.5541799102723598\n",
      "tensor(0.2745, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.5271808532252907\n",
      "tensor(0.3829, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.4689996438100934\n",
      "tensor(0.1135, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.4610202319175005\n",
      "tensor(0.2028, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.4464002800360322\n",
      "tensor(1.1271, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.3671783955208957\n",
      "tensor(0.0367, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.35648263378068806\n",
      "tensor(0.3925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.34026273308321836\n",
      "tensor(0.1412, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.3453915765695274\n",
      "tensor(0.2921, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.32724784079007807\n",
      "tensor(0.4730, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.30358684703707695\n",
      "tensor(0.4345, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.2905921445414424\n",
      "tensor(0.0566, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.2866789112985134\n",
      "tensor(0.2034, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.2900850367732346\n",
      "tensor(0.1836, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.24654499335214497\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.2418556446302682\n",
      "tensor(0.2511, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.212063760580495\n",
      "tensor(0.0459, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.2156342088058591\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.22538593995384873\n",
      "tensor(0.5257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.19950665466487408\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.20139817072078586\n",
      "tensor(0.4143, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.19249395778402686\n",
      "tensor(0.3760, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.17330022109672427\n",
      "tensor(0.2930, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.19076213066466152\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.15258703276049346\n",
      "tensor(0.3244, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.15354432572610677\n",
      "tensor(0.2587, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.16068784289993346\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.1307848805654794\n",
      "tensor(0.1684, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.1292416861280799\n",
      "tensor(0.0434, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.14161885687615722\n",
      "tensor(0.2588, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.12385696802753955\n",
      "tensor(0.0476, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.12542793858330697\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09683164025656879\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.12250908074900507\n",
      "tensor(0.2139, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09598802038934082\n",
      "tensor(0.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09214993311092258\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.10488219588063658\n",
      "tensor(0.5208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.08786340127699077\n",
      "tensor(0.1826, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.094366947831586\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09880342958495021\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.0948050521244295\n",
      "tensor(0.1130, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.10810658197384328\n",
      "tensor(0.1757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.10290823844261468\n",
      "tensor(0.2733, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.10725329015171155\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.08049576052697376\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09138837757520378\n",
      "tensor(0.1822, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09328700441867113\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.07771223009913228\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.09231985049555078\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.07264967784751207\n",
      "tensor(0.1027, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.057559386226348576\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.06365961486357265\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "0.0687178365897853\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "train(opt, bs, 10000, d_model, pos_enc, mask, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dd2fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer.eval()\n",
    "max_seq_len = MAX_SEQ_LEN\n",
    "bs = 1\n",
    "seq_len = random.randint(1, max_seq_len)\n",
    "enc_src, dec_src, target = generate_batch(bs, seq_len, d_model)\n",
    "original_seq = enc_src\n",
    "enc_src = F.one_hot(enc_src.to(torch.int64), num_classes=d_model).float()\n",
    "dec_src = F.one_hot(dec_src.to(torch.int64), num_classes=d_model).float()\n",
    "\n",
    "enc_src = enc_src.to(device)\n",
    "dec_src = dec_src.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "pe = pos_enc[0,:seq_len+1,:d_model]\n",
    "msk = mask[0, seq_len+1,: seq_len+1]\n",
    "pred = transformer(enc_src, dec_src, pe, None, msk)\n",
    "\n",
    "pred = pred.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f8f9cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 12])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "950e4ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64749db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  3,  5,  6,  8,  2,  4,  7, 10,  4,  6, 11]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c639f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(F.softmax(pred, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bd46619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  3,  5,  6,  8,  2,  4,  7, 10,  4,  6, 11]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef289807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  4., 10.,  7.,  4.,  2.,  8.,  6.,  5.,  3.,  6., 11.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371549e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17b283fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.8 ms ± 1.29 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%timeit -n 5 -r 5\n",
    "pred = transformer(enc_src, dec_src, pe, None, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "708dc2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77628"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in transformer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be5277e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderBlock(\n",
       "        (multi_head): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderBlock(\n",
       "        (multi_head_masked): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (multi_head_encoder): MultiHeadAttentionModule(\n",
       "          (linear): Linear(in_features=72, out_features=12, bias=False)\n",
       "          (a_modules): ModuleList(\n",
       "            (0): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (1): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (2): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (3): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (4): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "            (5): AttentionModule(\n",
       "              (Q): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (K): Linear(in_features=12, out_features=12, bias=False)\n",
       "              (V): Linear(in_features=12, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): FeedForward(\n",
       "          (l1): Linear(in_features=12, out_features=48, bias=True)\n",
       "          (l2): Linear(in_features=48, out_features=12, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (l1): Linear(in_features=12, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
